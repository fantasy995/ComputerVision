Table Of Contents:

+ [今日工作](#今日工作)
+ [核与核技巧](#核与核技巧)
  + [核方法](#核方法)
  + [核技巧](#核技巧)
+ [稀疏线性回归](#稀疏线性回归)
+ [贝叶斯线性回归拟合算法](#贝叶斯线性回归拟合算法)

### 今日工作

1、继续尝试完成贝叶斯线性回归拟合算法。

2、阅读《计算机视觉 模型、学习和推理》第8章回归模型的4，5节——核与核技巧，稀疏线性回归。



### 核与核技巧

由于非线性回归的贝叶斯方法有一个参数为`Z^t * Z` ，*z_i = f(x_i)* ，是一个比*x* 高维的向量，如果维度过高，拟合算法的计算机成本会很大。

#### 核方法

通过https://www.bilibili.com/video/av75892058 视频了解了什么是核方法。

定义： 在低维空间中不能线性分割的点集，通过转化为高维空间中的点集时，从而变为线性可分的，这就是核方法

但是这个映射可能会引起维度灾难，例如二维空间需要映射到五维空间，三维需要映射到19维。因此有映入了核技巧。

#### 核技巧

设有两个向量[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223057654-2111233806.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223057654-2111233806.png)和[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223108394-87800303.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223108394-87800303.png)
映射定义为：[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223345254-1729603565.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223345254-1729603565.png)
两个向量的映射为：
[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224124284-894094155.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224124284-894094155.png)

两个向量经过映射后的內积为：
[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223448094-1233601531.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116223448094-1233601531.png)
我们也可以有：
[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224234264-632179979.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224234264-632179979.png)

上面的两个式子是相似的， 实际上，经过维度缩放再加上常数维度就可以得到同样的效果。区别在哪里呢？
第一个是， **先通过映射将数据集映射到高维空间，再计算內积**
第二个是， **直接低维空间计算內积，而不需要显式的定义映射**
于是我们自然的可以定义**核函数** ：
[![img](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224750540-1067731375.png)](https://img2018.cnblogs.com/blog/1612966/201911/1612966-20191116224750540-1067731375.png)



### 稀疏线性回归

线性回归存在**过学习 ** 和**计算成本大** 的问题。

*x* 的维度中可能只有一部分对状态*ω* 有影响。不做修改，线性回归算法将在这些方向的梯度赋**非0值** 。

稀疏线性回归的目标是改造算法以求在一个大多数项为0的梯度向量。这样得到的分类器会更快，因为我们甚至不需要做所有测量，只需要测量梯度不为0的维度信息。

实现方法是向非0权重的维度增加一个惩罚项。



### 贝叶斯线性回归拟合算法

似然函数*Pr(ω|X,θ) = Norm_ω(X^Tφ, σ^2I)* 中的*σ* 参数最终会影响分布的均值，因此我们求*σ* 的局部最优解时存在一个范围，但是目前的算法无法支持我们实现这个需求，因此拟合结果不符合预期。

并且使用PyTorch的梯度下降算法时，*σ* 一直朝一个方向变化，这个最终值与书中提供的参考值相差很大，目前正在分析原因。