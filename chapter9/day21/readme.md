Table Of Contents:

+ [今日工作](#今日工作)
+ [核逻辑回归](#核逻辑回归)
+ [相关向量分类](#相关向量分类)



### 今日工作

1、阅读《计算机视觉 模型、学习和推理》第9章（分类模型），9.5节（核逻辑回归），9.6节（相关向量分类）。



### 核逻辑回归

经过之前的学习，可以看到第八章（回归模型)与第九章（分类模型）许多内容是相似的，只是分类模型状态是离散的，状态*ω* 的分布参数再0-1之间，所以引入了一个非线性函数。

在9.2节，为了得到非线性的分类边界，我们将*x* 进行非线性的变化*z=f[x]* 。

典型的非线性变化有：

![](https://github.com/fantasy995/ComputerVision/blob/main/images/Snipaste_2020-11-04_20-22-59.png?raw=true)

*z_k* 表示*z* 第*k* 个维度的值，因此我们可以拓展到任意的维度。

而预测分布的最终表达式中需要计算*z_i^T z_j* 的内积。

在为了减少计算强度，我们引入了核函数，即不计算中间变量*z* ，而直接使用*x_i* 和*x_j* 计算*z_i^T z_j* 的内积。

关于*x* 的非线性函数的参数同样影响最终模型的效果。



### 相关向量分类

相关向量回归是为了进一步降低计算的复杂程度。

并不是要降低训练过程的计算复杂度，而是训练之后我们可以去除掉很多样本，得到的分类器是基于部分样本的，提高了预测效率。

同样，在第8章（回归模型）中也提到了这个方法，名称叫做相关向量回归，只有一词之差。

总而言之，该方法是要寻找全体样本中最关键的子集，代表整个数据集，并通过*ψ* 参数设置每个样本的权重。



