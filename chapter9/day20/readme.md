Table Of Contents:

+ [今日工作](#今日工作)
+ [非线性逻辑回归](#非线性逻辑回归)
+ [对偶逻辑回归](#对偶逻辑回归)



### 今日工作

1、阅读《计算机视觉 模型、学习和推理》第9章（分类模型）9.3节（非线性逻辑回归），9.4节（对偶逻辑回归）。



### 非线性逻辑回归

今天阅读这里的时候对第8章（回归模型）提到的线性回归，本章开头提到的逻辑回归，以及今天阅读的非线性逻辑回归的分类感到困惑，以下是我的一些理解。

1、在第八章我们学习了线性回归和非线性回归模型。线性回归模型是后验分布*Pr(ω|Φ)* 中*Φ* 是通过关于*x* 的线性函数得到的，而在非线性回归模型中，我们将这*x* 与*Φ* 的映射关系调整为非线性。 

同时，*ω* 是连续的。

2、在第九章，我们最开始学习了逻辑回归模型，逻辑回归实际上是分类模型，*ω* 是离散的。适用离散分布的概率密度函数有伯努利分布（二元）和分类分布（多值），而这些分布的参数都需要一个非线性函数将参数映射到0-1之间。

3、而在9.1和9.2两节，我们得到的决策边界都是线性的，例如二元数据*x* 在空间中是用直线分成不同类别的。3

但是线性决策边界不符合实际中的很多问题，为了创造非线性决策边界，我们需要对数据*x* 进行非线性的变换（例如8.3节提到的非线性回归）。这个变换的表示同样为*z = f[x]* 。

除此之外，模型的拟合方法与逻辑回归相同。



### 对偶逻辑回归

在逻辑回归和非线性逻辑回归中，*x* 或*z* 的维度为*D\*1* ，*Φ* 的维度为*D\*1* 。即*x* 或*z* 的每个维度都有一个梯度。如果维度过高，迭代过程会很缓慢。

对偶参数化过程为:

![](https://github.com/fantasy995/ComputerVision/blob/main/images/Snipaste_2020-11-03_21-09-22.png?raw=true)

*ψ* 是*I\*1* 维的变量，*I* 小于*D* ，这样，我们就降低了要拟合的参数的维度。

同时，*ψ* 中的每一个数可以理解为每个数据样本的权重。

那么，逻辑回归模型为：

![](https://github.com/fantasy995/ComputerVision/blob/main/images/Snipaste_2020-11-03_21-12-15.png?raw=true)

同样，我们可以用似然法和贝叶斯方法拟合这个模型。

